{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(595212, 198) (892816, 198)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "\n",
    "# Regularized Greedy Forest\n",
    "from rgf.sklearn import RGFClassifier     # https://github.com/fukatani/rgf_python\n",
    "\n",
    "\n",
    "train = pd.read_csv('./input_original_data/porto_train.csv')\n",
    "test = pd.read_csv('./input_original_data/porto_test.csv')\n",
    "\n",
    "#train=train[:3000]\n",
    "#test=test[:3000]\n",
    "\n",
    "# Preprocessing \n",
    "id_test = test['id'].values\n",
    "target_train = train['target'].values\n",
    "\n",
    "train = train.drop(['target','id'], axis = 1)\n",
    "test = test.drop(['id'], axis = 1)\n",
    "\n",
    "\n",
    "col_to_drop = train.columns[train.columns.str.startswith('ps_calc_')]\n",
    "train = train.drop(col_to_drop, axis=1)  \n",
    "test = test.drop(col_to_drop, axis=1)  \n",
    "\n",
    "\n",
    "train = train.replace(-1, np.nan)\n",
    "test = test.replace(-1, np.nan)\n",
    "\n",
    "\n",
    "cat_features = [a for a in train.columns if a.endswith('cat')]\n",
    "\n",
    "for column in cat_features:\n",
    "    temp = pd.get_dummies(pd.Series(train[column]))\n",
    "    train = pd.concat([train,temp],axis=1)\n",
    "    train = train.drop([column],axis=1)\n",
    "    \n",
    "for column in cat_features:\n",
    "    temp = pd.get_dummies(pd.Series(test[column]))\n",
    "    test = pd.concat([test,temp],axis=1)\n",
    "    test = test.drop([column],axis=1)\n",
    "\n",
    "\n",
    "print(train.values.shape, test.values.shape)\n",
    "\n",
    "\n",
    "def eval_gini(y_true, y_prob):\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_true = y_true[np.argsort(y_prob)]\n",
    "    ntrue = 0\n",
    "    gini = 0\n",
    "    delta = 0\n",
    "    n = len(y_true)\n",
    "    for i in range(n-1, -1, -1):\n",
    "        y_i = y_true[i]\n",
    "        ntrue += y_i\n",
    "        gini += y_i * delta\n",
    "        delta += 1 - y_i\n",
    "    gini = 1 - 2 * gini / (ntrue * (n - ntrue))\n",
    "    return gini\n",
    "\n",
    "\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_splits, base_models):\n",
    "        self.n_splits = n_splits\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        folds = list(StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=2016).split(X, y))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "\n",
    "            S_test_i = np.zeros((T.shape[0], self.n_splits))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "#                y_holdout = y[test_idx]\n",
    "\n",
    "                print (\"Fit %s fold %d\" % (str(clf).split('(')[0], j+1))\n",
    "                clf.fit(X_train, y_train)\n",
    "#                cross_score = cross_val_score(clf, X_train, y_train, cv=3, scoring='roc_auc')\n",
    "#                print(\"    cross_score: %.5f\" % (cross_score.mean()))\n",
    "                y_pred = clf.predict_proba(X_holdout)[:,1]                \n",
    "\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict_proba(T)[:,1]\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(axis=1)\n",
    "            print( \"\\nGini for full training set:\" )\n",
    "            print(eval_gini(y,S_train[:,i]))\n",
    "            \n",
    "            \n",
    "        S_train = pd.DataFrame(S_train)\n",
    "        S_test = pd.DataFrame(S_test)\n",
    "            \n",
    "        S_train.columns = self.base_models\n",
    "        S_test.columns = self.base_models\n",
    "\n",
    "        return S_train, S_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "# LightGBM params\n",
    "lgb_params = {}\n",
    "lgb_params['learning_rate'] = 0.02\n",
    "lgb_params['n_estimators'] = 650\n",
    "lgb_params['max_bin'] = 10\n",
    "lgb_params['subsample'] = 0.8\n",
    "lgb_params['subsample_freq'] = 10\n",
    "lgb_params['colsample_bytree'] = 0.8   \n",
    "lgb_params['min_child_samples'] = 500\n",
    "lgb_params['seed'] = 99\n",
    "\n",
    "\n",
    "lgb_params2 = {}\n",
    "lgb_params2['n_estimators'] = 1090\n",
    "lgb_params2['learning_rate'] = 0.02\n",
    "lgb_params2['colsample_bytree'] = 0.3   \n",
    "lgb_params2['subsample'] = 0.7\n",
    "lgb_params2['subsample_freq'] = 2\n",
    "lgb_params2['num_leaves'] = 16\n",
    "lgb_params2['seed'] = 99\n",
    "\n",
    "\n",
    "lgb_params3 = {}\n",
    "lgb_params3['n_estimators'] = 1100\n",
    "lgb_params3['max_depth'] = 4\n",
    "lgb_params3['learning_rate'] = 0.02\n",
    "lgb_params3['seed'] = 99\n",
    "\n",
    "\n",
    "# RandomForest params\n",
    "#rf_params = {}\n",
    "#rf_params['n_estimators'] = 200\n",
    "#rf_params['max_depth'] = 6\n",
    "#rf_params['min_samples_split'] = 70\n",
    "#rf_params['min_samples_leaf'] = 30\n",
    "\n",
    "\n",
    "# ExtraTrees params\n",
    "#et_params = {}\n",
    "#et_params['n_estimators'] = 155\n",
    "#et_params['max_features'] = 0.3\n",
    "#et_params['max_depth'] = 6\n",
    "#et_params['min_samples_split'] = 40\n",
    "#et_params['min_samples_leaf'] = 18\n",
    "\n",
    "\n",
    "# XGBoost params\n",
    "#xgb_params = {}\n",
    "#xgb_params['objective'] = 'binary:logistic'\n",
    "#xgb_params['learning_rate'] = 0.04\n",
    "#xgb_params['n_estimators'] = 490\n",
    "#xgb_params['max_depth'] = 4\n",
    "#xgb_params['subsample'] = 0.9\n",
    "#xgb_params['colsample_bytree'] = 0.9  \n",
    "#xgb_params['min_child_weight'] = 10\n",
    "\n",
    "\n",
    "# CatBoost params\n",
    "#cat_params = {}\n",
    "#cat_params['iterations'] = 900\n",
    "#cat_params['depth'] = 8\n",
    "#cat_params['rsm'] = 0.95\n",
    "#cat_params['learning_rate'] = 0.03\n",
    "#cat_params['l2_leaf_reg'] = 3.5  \n",
    "#cat_params['border_count'] = 8\n",
    "#cat_params['gradient_iterations'] = 4\n",
    "\n",
    "\n",
    "# Regularized Greedy Forest params\n",
    "#rgf_params = {}\n",
    "#rgf_params['max_leaf'] = 2000\n",
    "#rgf_params['learning_rate'] = 0.5\n",
    "#rgf_params['algorithm'] = \"RGF_Sib\"\n",
    "#rgf_params['test_interval'] = 100\n",
    "#rgf_params['min_samples_leaf'] = 3 \n",
    "#rgf_params['reg_depth'] = 1.0\n",
    "#rgf_params['l2'] = 0.5  \n",
    "#rgf_params['sl2'] = 0.005\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 3\n",
      "\n",
      "Gini for full training set:\n",
      "0.282893853809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n",
      "/Users/kenji_tachibana/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/lightgbm/sklearn.py:282: LGBMDeprecationWarning: The `seed` parameter is deprecated and will be removed in next version. Please use `random_state` instead.\n",
      "  'Please use `random_state` instead.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LGBMClassifier fold 3\n",
      "\n",
      "Gini for full training set:\n",
      "0.282469202091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lgb_model = LGBMClassifier(**lgb_params)\n",
    "lgb_model2 = LGBMClassifier(**lgb_params2)\n",
    "lgb_model3 = LGBMClassifier(**lgb_params3)\n",
    "\n",
    "#rf_model = RandomForestClassifier(**rf_params)\n",
    "\n",
    "#et_model = ExtraTreesClassifier(**et_params)\n",
    "        \n",
    "#xgb_model = XGBClassifier(**xgb_params)\n",
    "\n",
    "#cat_model = CatBoostClassifier(**cat_params)\n",
    "\n",
    "#rgf_model = RGFClassifier(**rgf_params) \n",
    "\n",
    "#gb_model = GradientBoostingClassifier(max_depth=5)\n",
    "\n",
    "#ada_model = AdaBoostClassifier()\n",
    "\n",
    "log_model1 = LogisticRegression()\n",
    "log_model2 = XGBClassifier()\n",
    "\n",
    "\n",
    "\n",
    "#Lv1ensemble     \n",
    "stack1 = Ensemble(n_splits=3,\n",
    "                 base_models = (lgb_model, lgb_model2))              \n",
    "s1_train,s1_test = stack1.fit_predict(train, target_train, test)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit LogisticRegression fold 1\n",
      "Fit LogisticRegression fold 2\n",
      "Fit LogisticRegression fold 3\n",
      "\n",
      "Gini for full training set:\n",
      "0.284484436342\n",
      "Fit XGBClassifier fold 1\n",
      "Fit XGBClassifier fold 2\n",
      "Fit XGBClassifier fold 3\n",
      "\n",
      "Gini for full training set:\n",
      "0.28312770299\n"
     ]
    }
   ],
   "source": [
    "#Lv2ensemble\n",
    "stack2 = Ensemble(n_splits=3,\n",
    "                 base_models = (log_model1,log_model2))              \n",
    "s2_train,s2_test = stack2.fit_predict(s1_train, target_train, s1_test)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0.028959\n",
       "1         0.027455\n",
       "2         0.025260\n",
       "3         0.015649\n",
       "4         0.035432\n",
       "5         0.047277\n",
       "6         0.018505\n",
       "7         0.036855\n",
       "8         0.050238\n",
       "9         0.053874\n",
       "10        0.029480\n",
       "11        0.023089\n",
       "12        0.039857\n",
       "13        0.046668\n",
       "14        0.046542\n",
       "15        0.024077\n",
       "16        0.025719\n",
       "17        0.050464\n",
       "18        0.014914\n",
       "19        0.055201\n",
       "20        0.035974\n",
       "21        0.049541\n",
       "22        0.055733\n",
       "23        0.015890\n",
       "24        0.025344\n",
       "25        0.026566\n",
       "26        0.083873\n",
       "27        0.043104\n",
       "28        0.027693\n",
       "29        0.016815\n",
       "            ...   \n",
       "892786    0.015736\n",
       "892787    0.034619\n",
       "892788    0.033705\n",
       "892789    0.032678\n",
       "892790    0.033499\n",
       "892791    0.025594\n",
       "892792    0.027698\n",
       "892793    0.038002\n",
       "892794    0.023505\n",
       "892795    0.032331\n",
       "892796    0.059384\n",
       "892797    0.067647\n",
       "892798    0.040592\n",
       "892799    0.081336\n",
       "892800    0.025653\n",
       "892801    0.029072\n",
       "892802    0.028188\n",
       "892803    0.032094\n",
       "892804    0.047403\n",
       "892805    0.035946\n",
       "892806    0.024496\n",
       "892807    0.022217\n",
       "892808    0.024798\n",
       "892809    0.015601\n",
       "892810    0.025625\n",
       "892811    0.084086\n",
       "892812    0.044082\n",
       "892813    0.040718\n",
       "892814    0.023615\n",
       "892815    0.031439\n",
       "Length: 892816, dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def weight_ensemble(test,weight):\n",
    "    weighted_test = []\n",
    "    weight_list=np.zeros([test.shape[0],test.shape[1]])   \n",
    "    for i,w in enumerate(weight):\n",
    "        weight_list[:,i]= w/sum(weight)\n",
    "\n",
    "    weighted_test = (test * weight_list).sum(axis=1)\n",
    "    return weighted_test\n",
    "\n",
    "weight_ensemble(s2_test,[0.2,0.8])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.4,  1.2],\n",
       "       [ 0.4,  1.2],\n",
       "       [ 0.4,  1.2]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=np.array([[1,2],[1,2],[1,2]])\n",
    "y=np.zeros([x.shape[0],x.shape[1]])\n",
    "y[:,0]= 0.4\n",
    "y[:,1]= 0.6\n",
    "\n",
    "x*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6d2a107d05f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'id'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred_mean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stacked_LV2.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred_mean' is not defined"
     ]
    }
   ],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub['id'] = id_test\n",
    "sub['target'] = y_pred_mean\n",
    "sub.to_csv('stacked_LV2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "S2_test = np.zeros((T.shape[0], len(self.stackers)))\n",
    "        \n",
    "        for k,stacker in enumerate(self.stackers):\n",
    "\n",
    "            results = cross_val_score(stacker, S_train, y, cv=3, scoring='roc_auc')\n",
    "            print(\"Stacker score: %.5f\" % (results.mean()))\n",
    "        \n",
    "            stacker.fit(S_train, y)\n",
    "            S2_test[:, k] = stacker.predict_proba(S_test)[:,1]\n",
    "            \n",
    "        return S2_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
